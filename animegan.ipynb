{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob as gl\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\n\nclass AnimeDataSet(Dataset):\n    def __init__(self, root='', mode='',trans=None,trans_gray=None):\n        super().__init__()\n        self.transform = transforms.Compose(trans)\n        self.transform_gray=transforms.Compose(trans_gray)\n        self.source_path = os.path.join(root, \"source/*\")\n        self.style_path = os.path.join(root, f\"{mode}/style/*\")\n        self.smooth_path = os.path.join(root, f\"{mode}/smooth/*\")\n\n        self.list_style = gl.glob(self.style_path)\n        self.list_smooth = gl.glob(self.smooth_path)\n        self.list_source = gl.glob(self.source_path)\n\n    def __getitem__(self, index):\n        data = {}\n        style_path = random.choice(self.list_style)\n        smooth_path = random.choice(self.list_smooth)\n        img_path = random.choice(self.list_source)\n        style = Image.open(style_path).convert('RGB')\n        style_gray=Image.open(style_path).convert('L')\n        smooth_gray = Image.open(smooth_path).convert('L')\n        img = Image.open(img_path).convert('RGB')\n        img = self.transform(img)\n        img_A = self.transform(style)\n        img_B = self.transform_gray(style_gray)\n        img_C = self.transform_gray(smooth_gray)\n        img_B=img_B.squeeze(0)\n        img_C = img_C.squeeze(0)\n        img_B = np.stack([img_B, img_B, img_B], axis=0)\n        img_C = np.stack([img_C, img_C, img_C], axis=0)\n        data.update({'source': img, 'style': img_A, 'style_gray':img_B,'smooth_gray':img_C})\n        return data\n\n    def __len__(self):\n        return max(len(self.list_style), len(self.list_smooth),len(self.list_source))\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport cv2\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef gram(input):\n    b, c, w, h = input.size()\n    x = input.view(b * c, w * h)\n    G = torch.mm(x, x.T)\n    return G.div(b * c * w * h)\ndef rgb_to_yuv(image,x):\n    image = (image + 1.0) / 2.0\n    yuv_img = torch.tensordot(\n        image,\n        x,\n        dims=([image.ndim - 3], [0]))\n    return yuv_img\n\n\ndef divisible(dim):\n    width, height = dim\n    return width - (width % 32), height - (height % 32)\n\n\ndef resize_image(image, width=None, height=None, inter=cv2.INTER_AREA):\n    dim = None\n    h, w = image.shape[:2]\n\n    if width and height:\n        return cv2.resize(image, divisible((width, height)),  interpolation=inter)\n\n    if width is None and height is None:\n        return cv2.resize(image, divisible((w, h)),  interpolation=inter)\n\n    if width is None:\n        r = height / float(h)\n        dim = (int(w * r), height)\n\n    else:\n        r = width / float(w)\n        dim = (width, int(h * r))\n\n    return cv2.resize(image, divisible(dim), interpolation=inter)\n\nimport time as t\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n#初始化\ndef initialize_weights(net):\n    for m in net.modules():\n        try:\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.ConvTranspose2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        except Exception as e:\n            # print(f'SKip layer {m}, {e}')\n            pass\n\n#时间转化\ndef time_change(time):\n    new_time = t.localtime(time)\n    new_time = t.strftime(\"%Hh%Mm%Ss\", new_time)\n    return new_time\n#归一化\ndef  denorm(x):\n    x=(x* 0.5+ 0.5)*255.0\n    return x.cpu().detach().numpy().transpose(1,2,0)\ndef RGB2BGR(x):\n    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n#创建文件目录\ndef check_folder(log_dir):\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n#图片池\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\ndef compute_data_mean(data_folder):\n    if not os.path.exists(data_folder):\n        raise FileNotFoundError(f'Folder {data_folder} does not exits')\n\n    image_files = os.listdir(data_folder)\n    total = np.zeros(3)\n\n    print(f\"Compute mean (R, G, B) from {len(image_files)} images\")\n\n    for img_file in tqdm(image_files):\n        path = os.path.join(data_folder, img_file)\n        image = cv2.imread(path)\n        total += image.mean(axis=(0, 1))\n\n    channel_mean = total / len(image_files)\n    mean = np.mean(channel_mean)\n\n    return mean - channel_mean[...,::-1]  # Convert to BGR for training","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import spectral_norm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DownConv(nn.Module):\n\n    def __init__(self, channels, bias=False):\n        super(DownConv, self).__init__()\n\n        self.conv1 = SeparableConv2D(channels, channels, stride=2, bias=bias)\n        self.conv2 = SeparableConv2D(channels, channels, stride=1, bias=bias)\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out2 = F.interpolate(x, scale_factor=0.5, mode='bilinear')\n        out2 = self.conv2(out2)\n\n        return out1 + out2\n\n\nclass UpConv(nn.Module):\n    def __init__(self, channels, bias=False):\n        super(UpConv, self).__init__()\n\n        self.conv = SeparableConv2D(channels, channels, stride=1, bias=bias)\n\n    def forward(self, x):\n        out = F.interpolate(x, scale_factor=2.0, mode='bilinear')\n        out = self.conv(out)\n\n        return out\n\n\nclass SeparableConv2D(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, bias=False):\n        super(SeparableConv2D, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3,\n            stride=stride, padding=1, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels,\n            kernel_size=1, stride=1, bias=bias)\n        # self.pad =\n        self.ins_norm1 = nn.InstanceNorm2d(in_channels)\n        self.activation1 = nn.LeakyReLU(0.2, True)\n        self.ins_norm2 = nn.InstanceNorm2d(out_channels)\n        self.activation2 = nn.LeakyReLU(0.2, True)\n\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.ins_norm1(out)\n        out = self.activation1(out)\n\n        out = self.pointwise(out)\n        out = self.ins_norm2(out)\n\n        return self.activation2(out)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n        super(ConvBlock, self).__init__()\n\n        self.conv = nn.Conv2d(channels, out_channels,\n            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n        self.ins_norm = nn.InstanceNorm2d(out_channels)\n        self.activation = nn.LeakyReLU(0.2, True)\n\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.ins_norm(out)\n        out = self.activation(out)\n\n        return out\n\n\nclass InvertedResBlock(nn.Module):\n    def __init__(self, channels=256, out_channels=256, expand_ratio=2, bias=False):\n        super(InvertedResBlock, self).__init__()\n        bottleneck_dim = round(expand_ratio * channels)\n        self.conv_block = ConvBlock(channels, bottleneck_dim, kernel_size=1, stride=1, padding=0, bias=bias)\n        self.depthwise_conv = nn.Conv2d(bottleneck_dim, bottleneck_dim,\n            kernel_size=3, groups=bottleneck_dim, stride=1, padding=1, bias=bias)\n        self.conv = nn.Conv2d(bottleneck_dim, out_channels,\n            kernel_size=1, stride=1, bias=bias)\n\n        self.ins_norm1 = nn.InstanceNorm2d(out_channels)\n        self.ins_norm2 = nn.InstanceNorm2d(out_channels)\n        self.activation = nn.LeakyReLU(0.2, True)\n\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.conv_block(x)\n        out = self.depthwise_conv(out)\n        out = self.ins_norm1(out)\n        out = self.activation(out)\n        out = self.conv(out)\n        out = self.ins_norm2(out)\n\n        return out + x\n\nclass Generator(nn.Module):\n    def __init__(self, dataset=''):\n        super(Generator, self).__init__()\n        self.name = f'generator_{dataset}'\n        bias = False\n\n        self.encode_blocks = nn.Sequential(\n            ConvBlock(3, 64, bias=bias),\n            ConvBlock(64, 128, bias=bias),\n            DownConv(128, bias=bias),\n            ConvBlock(128, 128, bias=bias),\n            SeparableConv2D(128, 256, bias=bias),\n            DownConv(256, bias=bias),\n            ConvBlock(256, 256, bias=bias),\n        )\n\n        self.res_blocks = nn.Sequential(\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n        )\n\n        self.decode_blocks = nn.Sequential(\n            ConvBlock(256, 128, bias=bias),\n            UpConv(128, bias=bias),\n            SeparableConv2D(128, 128, bias=bias),\n            ConvBlock(128, 128, bias=bias),\n            UpConv(128, bias=bias),\n            ConvBlock(128, 64, bias=bias),\n            ConvBlock(64, 64, bias=bias),\n            nn.Conv2d(64, 3, kernel_size=1, stride=1, padding=0, bias=bias),\n            nn.Tanh(),\n        )\n\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.encode_blocks(x)\n        out = self.res_blocks(out)\n        img = self.decode_blocks(out)\n\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self,  args):\n        super(Discriminator, self).__init__()\n        self.name = f'discriminator_{args.dataset}'\n        self.bias = False\n        channels = 32\n\n        layers = [\n            nn.Conv2d(3, channels, kernel_size=3, stride=1, padding=1, bias=self.bias),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        for i in range(args.d_layers):\n            layers += [\n                nn.Conv2d(channels, channels * 2, kernel_size=3, stride=2, padding=1, bias=self.bias),\n                nn.LeakyReLU(0.2, True),\n                nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=1, padding=1, bias=self.bias),\n                nn.InstanceNorm2d(channels * 4),\n                nn.LeakyReLU(0.2, True),\n            ]\n            channels *= 4\n\n        layers += [\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=self.bias),\n            nn.InstanceNorm2d(channels),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(channels, 1, kernel_size=3, stride=1, padding=1, bias=self.bias),\n        ]\n\n        if args.use_sn:\n            for i in range(len(layers)):\n                if isinstance(layers[i], nn.Conv2d):\n                    layers[i] = spectral_norm(layers[i])\n\n        self.discriminate = nn.Sequential(*layers)\n\n        initialize_weights(self)\n\n    def forward(self, img):\n        return self.discriminate(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.lib.arraysetops import isin\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.nn as nn\nimport torch\nfrom PIL import Image\nimport numpy as np\n\n\n#VGG19\nclass VGG19(nn.Module):\n    def __init__(self,vgg_mean,vgg_std):\n        super(VGG19, self).__init__()\n        self.vgg19 = self.get_vgg19().eval()\n        self.mean = vgg_mean.view(-1, 1 ,1)\n        self.std = vgg_std.view(-1, 1, 1)\n\n    def forward(self, x):\n        return self.vgg19(self.normalize_vgg(x))\n\n\n    @staticmethod\n    def get_vgg19(last_layer='conv4_4'):\n        vgg = models.vgg19(pretrained=torch.cuda.is_available()).features\n        model_list = []\n\n        i = 0\n        j = 1\n        for layer in vgg.children():\n            if isinstance(layer, nn.MaxPool2d):\n                i = 0\n                j += 1\n\n            elif isinstance(layer, nn.Conv2d):\n                i += 1\n\n            name = f'conv{j}_{i}'\n\n            if name == last_layer:\n                model_list.append(layer)\n                break\n\n            model_list.append(layer)\n\n\n        model = nn.Sequential(*model_list)\n        return model\n    def normalize_vgg(self, image):\n        image = (image + 1.0) / 2.0\n        return (image - self.mean) / self.std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom torch import nn\nfrom torch import optim\nimport itertools\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.nn.functional import interpolate\nfrom torchvision.transforms import InterpolationMode\nimport cv2\nimport numpy as np\n\n\nimport time as t\nfrom tqdm import tqdm\n\nclass AnimeGANs(object):\n    def __init__(self,args):\n        #定义配置\n        self.cpu_count=args.cpu_count\n        self.test_dir=args.test_dir\n        self.istrain=args.istrain\n        self.istest=args.istest\n        self.init_train=args.init_train\n        self.retrain=args.retrain\n        self.epochs=args.epochs\n        self.init_epochs=args.init_epochs\n        self.data_dir=args.data_dir\n        self.dataset=args.dataset\n        self.result_dir=args.result_dir\n        self.save_interval=args.save_interval\n        self.checkpoint_dir=args.checkpoint_dir\n        self.save_image_dir=args.save_image_dir\n        self.G_lr=args.lr_g\n        self.D_lr = args.lr_d\n        self.decay_g=args.decay_g\n        self.decay_d=args.decay_d\n        self.init_lr=args.init_lr\n        self.batch_size=args.batch_size\n        self.d_noise=args.d_noise\n        self.device=args.device\n        self.vgg_mean = torch.tensor([0.485, 0.456, 0.406]).float().to(self.device)\n        self.vgg_std = torch.tensor([0.229, 0.224, 0.225]).float().to(self.device)\n        \n        #定义模型\n        self.G=Generator(dataset=args.dataset).to(self.device)\n        self.D=Discriminator(args).to(self.device)\n        self.vgg19=VGG19(self.vgg_mean,self.vgg_std).to(self.device)\n#         self.vgg19.load_state_dict(torch.load('/kaggle/input/prevgg19/vgg19.pth'))\n        #定义优化器\n        if self.init_train:\n            self.G_optim = optim.Adam(self.G.parameters(), lr=self.init_lr, betas=(0.5, 0.999))\n        else:\n            self.G_optim=optim.Adam(self.G.parameters(),lr=self.G_lr,betas=(0.5, 0.999))\n        self.D_optim=optim.Adam(self.D.parameters(), lr=self.D_lr, betas=(0.5, 0.999))\n        #定义损失函数\n        self.huber = nn.SmoothL1Loss().to(self.device)\n        self.content_loss = nn.L1Loss().to(self.device)\n        self.gram_loss = nn.L1Loss().to(self.device)\n        self.color_loss = nn.L1Loss().to(self.device)\n        self.gan_loss = args.gan_loss\n        self.wadvg = args.wadvg\n        self.wadvd = args.wadvd\n        self.wcon = args.wcon\n        self.wgra = args.wgra\n        self.wcol = args.wcol\n        self.adv_type = args.gan_loss\n        #noise\n        self.gaussian_mean=torch.tensor(0.0)\n        self.gaussian_std=torch.tensor(0.1)\n        self._rgb_to_yuv_kernel = torch.tensor([\n    [0.299, -0.14714119, 0.61497538],\n    [0.587, -0.28886916, -0.51496512],\n    [0.114, 0.43601035, -0.10001026]\n]).float().to(self.device)\n    #高斯噪声\n    def gaussian_noise(self):\n        return torch.normal(self.gaussian_mean, self.gaussian_std)\n    #数据加载。。。。\n    def load_data(self):\n        trans = [transforms.Resize(286, InterpolationMode.BICUBIC),\n                     transforms.CenterCrop(256),\n                     transforms.RandomHorizontalFlip(0.5),\n                     transforms.ToTensor(),\n                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n        trans_gray = [transforms.Resize(286, InterpolationMode.BICUBIC),\n                 transforms.CenterCrop(256),\n                 transforms.RandomHorizontalFlip(0.5),\n                 transforms.ToTensor(),\n                 transforms.Normalize((0.5), (0.5))]\n        data_loader = DataLoader(AnimeDataSet(root=self.data_dir,mode=self.dataset,trans=trans,trans_gray=trans_gray),\n                                  batch_size=self.batch_size, shuffle=True,num_workers=self.cpu_count,\n                                  pin_memory=True,drop_last=True)\n        return data_loader\n    #输入数据\n    def set_inputs(self,input):\n        self.source_img=input['source'].to(self.device)\n        self.style_img = input['style'].to(self.device)\n        self.stg_img = input['style_gray'].to(self.device)\n        self.smg_img = input['smooth_gray'].to(self.device)\n    #训练\n    def train(self):\n        print(\"training on 4090\")\n        print('=================train start!=========================')\n        data_loader=self.load_data()#读取图像\n        start_time = t.time()\n        for step in tqdm(range(1,41)):\n            if step>(self.epochs//2):\n                    self.G_optim.param_groups[0]['lr']-=self.G_lr/(self.epochs//2)\n                    self.D_optim.param_groups[0]['lr']-=self.D_lr/(self.epochs//2)\n            for i,data in enumerate(data_loader):\n                count=len(data_loader)\n                self.G.train()\n                self.set_inputs(data)\n                # train G with  content loss only\n                if  not self.init_train:\n                    #========================D=================\n                    self.D.train()\n                    self.D_optim.zero_grad()\n                    fake_img=self.G(self.source_img).detach()\n                    if self.d_noise:\n                        fake_img += self.gaussian_noise()\n                        self.style_img += self.gaussian_noise()\n                        self.stg_img += self.gaussian_noise()\n                        self.smg_img += self.gaussian_noise()\n                    fake_d = self.D(fake_img)\n                    real_anime_d = self.D(self.style_img)\n                    real_anime_gray_d = self.D(self.stg_img)\n                    real_anime_smg_gray_d = self.D(self.smg_img)\n                    #loss\n                    real_anime_d=torch.mean(torch.square(real_anime_d-1.0))\n                    fake_d=torch.mean(torch.square(fake_d))#lsgan\n                    real_anime_gray_d = torch.mean(torch.square(real_anime_gray_d))\n                    real_anime_smg_gray_d= torch.mean(torch.square(real_anime_smg_gray_d))\n                    loss_d=self.wadvd*(real_anime_d+fake_d+real_anime_gray_d+0.2*real_anime_smg_gray_d)\n                    loss_d.backward()\n                    self.D_optim.step()\n                    #=========================train g=================\n                    self.G_optim.zero_grad()\n                    fake_img=self.G(self.source_img)\n                    fake_d=self.D(fake_img)\n                    fake_teture=self.vgg19(fake_img)\n                    real_teture=self.vgg19(self.source_img)\n                    styg_teture=self.vgg19(self.stg_img)\n                    adv_loss=torch.mean(torch.square(fake_d - 1.0))\n                    con_loss=self.content_loss(fake_teture,real_teture)\n                    gram_loss=self.gram_loss(gram(styg_teture),gram(fake_teture))\n                    source_yuv=rgb_to_yuv(self.source_img,self._rgb_to_yuv_kernel )\n                    fake_yuv=rgb_to_yuv(fake_img,self._rgb_to_yuv_kernel )\n                    col_loss=(self.color_loss(source_yuv[:,:,:,0],fake_yuv[:,:,:,0])+self.huber(source_yuv[:,:,:,1],fake_yuv[:,:,:,1])+\\\n                             self.huber(source_yuv[:,:,:,2],fake_yuv[:,:,:,2]))\n                    loss_G=adv_loss*self.wadvg+con_loss*self.wcon+gram_loss*self.wgra+col_loss*self.wcol\n                    loss_G.backward()\n                    self.G_optim.step()\n                else:\n                    self.G_optim.zero_grad()\n                    fake_img = self.G(self.source_img)\n                    real_con = self.vgg19(self.source_img)\n                    fake_con = self.vgg19(fake_img)\n                    loss_con = self.content_loss(fake_con, real_con)\n                    loss_con.backward()\n                    self.G_optim.step()\n                t_end=t.time()\n                if self.init_train:\n                    print(\n                        f\"epoch:[{step}/{self.epochs}],iter:[{i+1}/{count}],loss_G:{loss_con},G_lr:{self.G_optim.param_groups[0]['lr']},time:{time_change(t_end -start_time)}\")\n                else:\n                    print(\n                        f\"epoch[{step}/{self.epochs}],iter[{i+1}/{count}],loss_G:{loss_G},loss_D:{loss_d},G_lr:{self.G_optim.param_groups[0]['lr']},D_lr:{self.D_optim.param_groups[0]['lr']},time:{time_change(t_end - start_time)}\")\n            if step%self.save_interval==0:\n                train_sample_num = 5\n                style= np.zeros((256 * 3, 0, 3))\n                self.G.eval(), self.D.eval()\n                data_loader=self.load_data()\n                for _ in range(train_sample_num):\n                    for i, data in tqdm(enumerate(data_loader)):\n                        break\n                    real_img = data['source'].to(self.device)\n                    style_img = data['style'].to(self.device)\n                    fake_img=self.G(real_img)  # 生成假图\n                    style = np.concatenate((style, np.concatenate((RGB2BGR(denorm(real_img[0])),\n                                                                     RGB2BGR(denorm(style_img[0])),\n                                                                     RGB2BGR(denorm(fake_img[0]))), 0)), 1)\n                cv2.imwrite(os.path.join(self.result_dir, self.dataset, 'img', 'anime_%06d.png' % step), style)\n                print(\"测试图像生成成功！\")\n                self.save_model()\n                # 保存模型\n\n    def save_model(self):\n        params = {}\n        params[\"G\"] = self.G.state_dict()\n        params[\"D\"] = self.D.state_dict()\n        torch.save(params, os.path.join(self.result_dir, self.dataset,self.checkpoint_dir,f'checkpoints_{self.dataset}.pth'))\n        print(\"保存模型成功！\")\n        # 加载模型\n\n    def load_model(self):\n        params = torch.load(os.path.join(self.test_dir, f'checkpoints_{self.dataset}.pth'))\n        self.G.load_state_dict(params['G'])\n        self.D.load_state_dict(params['D'])\n        print(\"加载模型成功！\")\n#保存生成图像\n    def test(self):\n        self.load_model()\n        data_loader=self.load_data()\n        self.G.eval()\n        for i ,data in tqdm(enumerate(data_loader)):\n            real_img=data['source'].to(self.device)\n            fake_img=self.G(real_img)# 生成假图\n            fake_img=RGB2BGR(denorm(fake_img[0]))\n            cv2.imwrite(os.path.join(self.result_dir, self.dataset, 'test/img','style_%06d.png' % i), fake_img)\n            if i==3000:\n                break\n        print(\"测试图像生成成功！\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport argparse\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default='Shinkai')\n    parser.add_argument('--data_dir', type=str, default='/kaggle/input/dataset/datasets')\n    parser.add_argument('--result_dir', type=str, default='results')\n    parser.add_argument('--epochs', type=int, default=100)\n    parser.add_argument('--cpu_count', type=int, default=1)\n    parser.add_argument('--init_epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=4)\n    parser.add_argument('--test_dir', type=str, default='/kaggle/input/skinkai50')\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n    parser.add_argument('--save_image_dir', type=str, default='img')\n    parser.add_argument('--gan_loss', type=str, default='lsgan', help='lsgan / hinge / bce')\n    parser.add_argument('--retrain', type=bool, default=True)\n    parser.add_argument('--istrain', type=bool, default=True)\n    parser.add_argument('--init_train', type=bool, default=False)\n    parser.add_argument('--istest', type=bool, default=False)\n    parser.add_argument('--use_sn', type=bool, default=True)\n    parser.add_argument('--save_interval', type=int, default=1)\n    parser.add_argument('--lr_g', type=float, default=0.00008)\n    parser.add_argument('--lr_d', type=float, default=0.00016)\n    parser.add_argument('--decay_g', type=float, default=4.800000000000004e-05)\n    parser.add_argument('--decay_d', type=float, default=9.600000000000008e-05)\n    parser.add_argument('--init_lr', type=float, default=0.0001)\n    parser.add_argument('--wadvg', type=float, default=10.0, help='Adversarial loss weight for G')\n    parser.add_argument('--wadvd', type=float, default=10.0, help='Adversarial loss weight for D')\n    parser.add_argument('--wcon', type=float, default=1.5, help='Content loss weight')\n    parser.add_argument('--wgra', type=float, default=3.0, help='Gram loss weight')\n    parser.add_argument('--wcol', type=float, default=30.0, help='Color loss weight')\n    parser.add_argument('--d_layers', type=int, default=3, help='Discriminator conv layers')\n    parser.add_argument('--d_noise', type=bool, default=True)\n    parser.add_argument('--device',type=str,default='cuda',choices=['cuda','cpu'])\n    return check_args(parser.parse_args(args=[]))\n\ndef check_args(args):\n    check_folder(os.path.join(args.result_dir, args.dataset, 'checkpoints'))\n    check_folder(os.path.join(args.result_dir, args.dataset, 'img'))\n    check_folder(os.path.join(args.result_dir, args.dataset, 'test'))\n    check_folder(os.path.join(args.result_dir, args.dataset, 'test', 'img'))\n    return args\n\n\ndef main():\n   args=parse_args()\n   gan=AnimeGANs(args)\n   if args.istrain:\n       if args.retrain:\n            gan.load_model()\n       print(f\"training on {args.device}\")\n       gan.train()\n       print(\"train haved finished\")\n   if args.istest:\n       gan.test()\n       print(\"test haved finished\")\nif __name__==\"__main__\":\n    main()\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\ndef file2zip(packagePath, zipPath):\n    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n    for path, dirNames, fileNames in os.walk(packagePath):\n        fpath = path.replace(packagePath, '')\n        for name in fileNames:\n            fullName = os.path.join(path, name)\n            name = fpath + '\\\\' + name\n            zip.write(fullName, name)\n    zip.close()\nif __name__ == \"__main__\":\n    # 文件夹路径\n    packagePath = './results'\n    zipPath = './model.zip'\n    if os.path.exists(zipPath):\n        os.remove(zipPath)\n    file2zip(packagePath, zipPath)\n    print(\"打包完成\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}